{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Stone/Downloads/iot_23_datasets_full/data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dir = os.path.realpath('..').replace('\\\\', '/')\n",
    "\n",
    "file_dir = dir + '/data/'\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positives = pd.read_csv(file_dir + 'df_negative.csv')\n",
    "df_negatives = pd.read_csv(file_dir + 'df_positive.csv')\n",
    "\n",
    "# print the shape of the dataframes\n",
    "print(df_positives.shape)\n",
    "print(df_negatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 50% of the positive samples to X_train_positive\n",
    "X_train_positive = df_positives.sample(frac=0.5, random_state=0)\n",
    "\n",
    "# in the rest 50% of the positive samples, select 20% to X_test_positive\n",
    "X_test_positive = df_positives.drop(X_train_positive.index)\n",
    "\n",
    "# drop 50% in X_test_positive\n",
    "X_test_positive = X_test_positive.sample(frac=0.5, random_state=0)\n",
    "\n",
    "# get the number of samples in X_test_positive and X_train_positive\n",
    "n_X_test_positive = X_test_positive.shape[0]\n",
    "n_X_train_positive = X_train_positive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n_X_train_positive/10 samples from the negative samples 10 times and add them to a list\n",
    "X_train_negative = []\n",
    "for i in range(10):\n",
    "    X_train_negative.append(df_negatives.sample(n=n_X_train_positive//10, random_state=i))\n",
    "\n",
    "# concatenate X_train_positive to each of the samples in X_train_negative list and get 10 X_train datasets\n",
    "X_train = []\n",
    "for i in range(10):\n",
    "    X_train.append(pd.concat([X_train_positive, X_train_negative[i]]))\n",
    "\n",
    "# select n_X_test_positive/10 samples from the negative samples 10 times and add them to a list\n",
    "X_test_negative = []\n",
    "for i in range(10):\n",
    "    X_test_negative.append(df_negatives.drop(X_train_negative[i].index).sample(n=n_X_test_positive//10, random_state=i))\n",
    "\n",
    "# concatenate X_test_positive to each of the samples in X_test_negative list and get 10 X_test datasets\n",
    "X_test = []\n",
    "for i in range(10):\n",
    "    X_test.append(pd.concat([X_test_positive, X_test_negative[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in X_train and X_test, select the 'target' as y_train and y_test\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(10):\n",
    "    y_train.append(X_train[i]['target'])\n",
    "    y_test.append(X_test[i]['target'])\n",
    "\n",
    "# in X_train and X_test, drop the 'target' column\n",
    "for i in range(10):\n",
    "    X_train[i] = X_train[i].drop(['target'], axis=1)\n",
    "    X_test[i] = X_test[i].drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.02 GiB for an array with shape (44, 6169710) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Stone\\Downloads\\iot_23_datasets_full\\src\\read.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m IsolationForest(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_samples\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, contamination\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m0.1\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                         bootstrap\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# use the model to predict the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stone/Downloads/iot_23_datasets_full/src/read.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_iforest.py:306\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_samples_ \u001b[39m=\u001b[39m max_samples\n\u001b[0;32m    305\u001b[0m max_depth \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(np\u001b[39m.\u001b[39mlog2(\u001b[39mmax\u001b[39m(max_samples, \u001b[39m2\u001b[39m))))\n\u001b[1;32m--> 306\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    307\u001b[0m     X,\n\u001b[0;32m    308\u001b[0m     y,\n\u001b[0;32m    309\u001b[0m     max_samples,\n\u001b[0;32m    310\u001b[0m     max_depth\u001b[39m=\u001b[39;49mmax_depth,\n\u001b[0;32m    311\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    312\u001b[0m     check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m )\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontamination \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    316\u001b[0m     \u001b[39m# 0.5 plays a special role as described in the original paper.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39m# we take the opposite as we consider the opposite of their score.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset_ \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_bagging.py:434\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39mn_more_estimators)\n\u001b[0;32m    432\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seeds \u001b[39m=\u001b[39m seeds\n\u001b[1;32m--> 434\u001b[0m all_results \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    435\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parallel_args()\n\u001b[0;32m    436\u001b[0m )(\n\u001b[0;32m    437\u001b[0m     delayed(_parallel_build_estimators)(\n\u001b[0;32m    438\u001b[0m         n_estimators[i],\n\u001b[0;32m    439\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    440\u001b[0m         X,\n\u001b[0;32m    441\u001b[0m         y,\n\u001b[0;32m    442\u001b[0m         sample_weight,\n\u001b[0;32m    443\u001b[0m         seeds[starts[i] : starts[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]],\n\u001b[0;32m    444\u001b[0m         total_n_estimators,\n\u001b[0;32m    445\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    446\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    447\u001b[0m     )\n\u001b[0;32m    448\u001b[0m     \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_jobs)\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    451\u001b[0m \u001b[39m# Reduce\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    453\u001b[0m     itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(t[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m all_results)\n\u001b[0;32m    454\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\_parallel_backends.py:595\u001b[0m, in \u001b[0;36mSafeFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    594\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 595\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    596\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    597\u001b[0m         \u001b[39m# We capture the KeyboardInterrupt and reraise it as\u001b[39;00m\n\u001b[0;32m    598\u001b[0m         \u001b[39m# something different, as multiprocessing does not\u001b[39;00m\n\u001b[0;32m    599\u001b[0m         \u001b[39m# interrupt processing for a KeyboardInterrupt\u001b[39;00m\n\u001b[0;32m    600\u001b[0m         \u001b[39mraise\u001b[39;00m WorkerInterrupt() \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_bagging.py:138\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[0;32m    135\u001b[0m         not_indices_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mindices_to_mask(indices, n_samples)\n\u001b[0;32m    136\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 138\u001b[0m     estimator_fit(X[:, features], y, sample_weight\u001b[39m=\u001b[39mcurr_sample_weight)\n\u001b[0;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     estimator_fit(X[indices][:, features], y[indices])\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.02 GiB for an array with shape (44, 6169710) and data type float64"
     ]
    }
   ],
   "source": [
    "# import isolation forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# import classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# build a model only use the first dataset in X_train and X_test\n",
    "model = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.1), max_features=1.0,\n",
    "                        bootstrap=False, random_state=42)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train[0])\n",
    "\n",
    "# use the model to predict the test set\n",
    "y_pred = model.predict(X_test[0])\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_pred, y_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
